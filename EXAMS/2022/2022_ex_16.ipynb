{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a661780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 TEACHER'S PROBLEM\n",
      "==================================================\n",
      "Network initialized: 2 inputs → 3 hidden → 3 outputs\n",
      "\n",
      "=== NETWORK STRUCTURE ===\n",
      "Input → Hidden weights (W1):\n",
      "[[-0.8 -0.1  0.4]\n",
      " [ 0.   0.9 -0.1]]\n",
      "Hidden bias (b1): [0. 0. 0.]\n",
      "\n",
      "Hidden → Output weights (W2):\n",
      "[[ 0.   0.1  0. ]\n",
      " [ 0.4 -0.2  0.8]\n",
      " [ 0.  -0.2 -0.2]]\n",
      "Output bias (b2): [0.4 0.5 0. ]\n",
      "\n",
      "=== FORWARD PASS ===\n",
      "Input: [1.  0.5]\n",
      "Hidden pre-activation (z1): [-0.8   0.35  0.35]\n",
      "Hidden activation (h): [0.   0.35 0.35]\n",
      "Output pre-activation (z2): [0.54 0.36 0.21]\n",
      "Output probabilities (ŷ): [0.39151295 0.3270191  0.28146795]\n",
      "Correct class: 1\n",
      "Loss: 1.117737\n",
      "\n",
      "🎯 TEACHER'S QUESTION:\n",
      "\n",
      "=== ∂L/∂w_12^(2) ===\n",
      "Location: Hidden unit 1 → Output unit 2\n",
      "Current weight: W2[1,2] = 0.8\n",
      "Target for output 2: 0 (✗ wrong class)\n",
      "Formula: (ŷ_2 - t_2) × h_1\n",
      "Values: (0.281468 - 0) × 0.350000\n",
      "Gradient: 0.098514\n",
      "Direction: 📈 Increase weight\n",
      "\n",
      "Teacher's MATLAB answer: h(2)*y(3) = 0.098514\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🔸 SIMPLE EXAMPLE\n",
      "==================================================\n",
      "Network initialized: 2 inputs → 2 hidden → 2 outputs\n",
      "\n",
      "=== NETWORK STRUCTURE ===\n",
      "Input → Hidden weights (W1):\n",
      "[[-0.5  0.3]\n",
      " [ 0.8 -0.2]]\n",
      "Hidden bias (b1): [0. 0.]\n",
      "\n",
      "Hidden → Output weights (W2):\n",
      "[[ 0.4 -0.6]\n",
      " [ 0.1  0.9]]\n",
      "Output bias (b2): [0. 0.]\n",
      "\n",
      "=== FORWARD PASS ===\n",
      "Input: [1.  0.7]\n",
      "Hidden pre-activation (z1): [0.06 0.16]\n",
      "Hidden activation (h): [0.06 0.16]\n",
      "Output pre-activation (z2): [0.04  0.108]\n",
      "Output probabilities (ŷ): [0.48300655 0.51699345]\n",
      "Correct class: 0\n",
      "Loss: 0.727725\n",
      "\n",
      "📍 SPECIFIC GRADIENTS:\n",
      "\n",
      "=== ∂L/∂w_01^(2) ===\n",
      "Location: Hidden unit 0 → Output unit 1\n",
      "Current weight: W2[0,1] = -0.6\n",
      "Target for output 1: 0 (✗ wrong class)\n",
      "Formula: (ŷ_1 - t_1) × h_0\n",
      "Values: (0.516993 - 0) × 0.060000\n",
      "Gradient: 0.031020\n",
      "Direction: 📈 Increase weight\n",
      "\n",
      "=== ∂L/∂w_10^(2) ===\n",
      "Location: Hidden unit 1 → Output unit 0\n",
      "Current weight: W2[1,0] = 0.1\n",
      "Target for output 0: 1 (✓ correct class)\n",
      "Formula: (ŷ_0 - t_0) × h_1\n",
      "Values: (0.483007 - 1) × 0.160000\n",
      "Gradient: -0.082719\n",
      "Direction: 📉 Decrease weight\n",
      "\n",
      "=== ∂L/∂w_10^(1) ===\n",
      "Location: Input unit 1 → Hidden unit 0\n",
      "Current weight: W1[1,0] = 0.8\n",
      "Step 1: ∂L/∂h_0 = -0.516993 (backprop from output)\n",
      "Step 2: ReLU'(z1_0) = 1 (z1_0 = 0.060000)\n",
      "Step 3: δ_0 = -0.516993 × 1 = -0.516993\n",
      "Step 4: Gradient = δ_0 × x_1 = -0.516993 × 0.700000\n",
      "Gradient: -0.361895\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🔧 YOUR CUSTOM NETWORK\n",
      "==================================================\n",
      "Network initialized: 3 inputs → 3 hidden → 2 outputs\n",
      "\n",
      "=== NETWORK STRUCTURE ===\n",
      "Input → Hidden weights (W1):\n",
      "[[-1.   0.5  0.2]\n",
      " [ 0.3 -0.8  0.6]\n",
      " [ 0.1  0.4 -0.3]]\n",
      "Hidden bias (b1): [0. 0. 0.]\n",
      "\n",
      "Hidden → Output weights (W2):\n",
      "[[ 0.7 -0.4]\n",
      " [ 0.2  0.9]\n",
      " [-0.5  0.1]]\n",
      "Output bias (b2): [ 0.1 -0.2]\n",
      "\n",
      "=== FORWARD PASS ===\n",
      "Input: [1.  0.4 0.8]\n",
      "Hidden pre-activation (z1): [-0.8  0.5  0.2]\n",
      "Hidden activation (h): [0.  0.5 0.2]\n",
      "Output pre-activation (z2): [0.1  0.27]\n",
      "Output probabilities (ŷ): [0.45760206 0.54239794]\n",
      "Correct class: 1\n",
      "Loss: 0.611755\n",
      "\n",
      "📍 YOUR GRADIENT:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid indices: from_hidden=3, to_output=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 362\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# Your custom network\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m my_net \u001b[38;5;241m=\u001b[39m \u001b[43mmy_custom_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎯 USAGE SUMMARY:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 339\u001b[0m, in \u001b[0;36mmy_custom_network\u001b[1;34m()\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# ✏️ CALCULATE THE GRADIENT YOU WANT:\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📍 YOUR GRADIENT:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 339\u001b[0m my_gradient \u001b[38;5;241m=\u001b[39m \u001b[43mmy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 1 = input→hidden, 2 = hidden→output\u001b[39;49;00m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Source unit index\u001b[39;49;00m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Destination unit index\u001b[39;49;00m\n\u001b[0;32m    343\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYour gradient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_gradient\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m my_net\n",
      "Cell \u001b[1;32mIn[4], line 108\u001b[0m, in \u001b[0;36mNetworkGradientCalculator.get_gradient\u001b[1;34m(self, layer, from_unit, to_unit)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust call forward_pass() first!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_output_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_unit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_unit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hidden_gradient(from_unit, to_unit)\n",
      "Cell \u001b[1;32mIn[4], line 119\u001b[0m, in \u001b[0;36mNetworkGradientCalculator._get_output_gradient\u001b[1;34m(self, from_hidden, to_output)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Check bounds\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_hidden \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_hidden \u001b[38;5;129;01mor\u001b[39;00m to_output \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid indices: from_hidden=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrom_hidden\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, to_output=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Formula: (ŷ_j - t_j) × h_i\u001b[39;00m\n\u001b[0;32m    122\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_output \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_class \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid indices: from_hidden=3, to_output=2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NetworkGradientCalculator:\n",
    "    \"\"\"\n",
    "    Define your network once, then calculate any gradient by specifying location\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_to_hidden_weights, hidden_to_output_weights, \n",
    "                 hidden_bias=None, output_bias=None):\n",
    "        \"\"\"\n",
    "        Initialize network with predefined weights\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_to_hidden_weights : list/array, shape (n_inputs, n_hidden)\n",
    "            Weights from input layer to hidden layer\n",
    "        hidden_to_output_weights : list/array, shape (n_hidden, n_outputs)  \n",
    "            Weights from hidden layer to output layer\n",
    "        hidden_bias : list/array, optional, shape (n_hidden,)\n",
    "            Bias terms for hidden layer\n",
    "        output_bias : list/array, optional, shape (n_outputs,)\n",
    "            Bias terms for output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store network architecture\n",
    "        self.W1 = np.array(input_to_hidden_weights)  # Input → Hidden\n",
    "        self.W2 = np.array(hidden_to_output_weights) # Hidden → Output\n",
    "        self.b1 = np.array(hidden_bias) if hidden_bias is not None else np.zeros(self.W1.shape[1])\n",
    "        self.b2 = np.array(output_bias) if output_bias is not None else np.zeros(self.W2.shape[1])\n",
    "        \n",
    "        # Network dimensions\n",
    "        self.n_inputs = self.W1.shape[0]\n",
    "        self.n_hidden = self.W1.shape[1] \n",
    "        self.n_outputs = self.W2.shape[1]\n",
    "        \n",
    "        # Forward pass storage\n",
    "        self.x = None\n",
    "        self.z1 = None  # Hidden pre-activation\n",
    "        self.h = None   # Hidden activation (after ReLU)\n",
    "        self.z2 = None  # Output pre-activation\n",
    "        self.y_hat = None  # Output probabilities (after softmax)\n",
    "        self.target_class = None\n",
    "        \n",
    "        print(f\"Network initialized: {self.n_inputs} inputs → {self.n_hidden} hidden → {self.n_outputs} outputs\")\n",
    "        self.show_network()\n",
    "    \n",
    "    def show_network(self):\n",
    "        \"\"\"Display the network structure and weights\"\"\"\n",
    "        print(\"\\n=== NETWORK STRUCTURE ===\")\n",
    "        print(f\"Input → Hidden weights (W1):\")\n",
    "        print(self.W1)\n",
    "        print(f\"Hidden bias (b1): {self.b1}\")\n",
    "        print(f\"\\nHidden → Output weights (W2):\")\n",
    "        print(self.W2)\n",
    "        print(f\"Output bias (b2): {self.b2}\")\n",
    "    \n",
    "    def forward_pass(self, inputs, correct_class):\n",
    "        \"\"\"\n",
    "        Perform forward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : list/array, input values\n",
    "        correct_class : int, index of correct output class (0-indexed)\n",
    "        \"\"\"\n",
    "        self.x = np.array(inputs)\n",
    "        self.target_class = correct_class\n",
    "        \n",
    "        # Hidden layer: z1 = x * W1 + b1, h = ReLU(z1)\n",
    "        self.z1 = np.dot(self.x, self.W1) + self.b1\n",
    "        self.h = np.maximum(0, self.z1)\n",
    "        \n",
    "        # Output layer: z2 = h * W2 + b2, y = softmax(z2)\n",
    "        self.z2 = np.dot(self.h, self.W2) + self.b2\n",
    "        exp_z2 = np.exp(self.z2 - np.max(self.z2))  # For numerical stability\n",
    "        self.y_hat = exp_z2 / np.sum(exp_z2)\n",
    "        \n",
    "        print(f\"\\n=== FORWARD PASS ===\")\n",
    "        print(f\"Input: {self.x}\")\n",
    "        print(f\"Hidden pre-activation (z1): {self.z1}\")\n",
    "        print(f\"Hidden activation (h): {self.h}\")\n",
    "        print(f\"Output pre-activation (z2): {self.z2}\")\n",
    "        print(f\"Output probabilities (ŷ): {self.y_hat}\")\n",
    "        print(f\"Correct class: {correct_class}\")\n",
    "        print(f\"Loss: {-np.log(self.y_hat[correct_class]):.6f}\")\n",
    "        \n",
    "        return self.y_hat\n",
    "    \n",
    "    def get_gradient(self, layer, from_unit, to_unit):\n",
    "        \"\"\"\n",
    "        Calculate gradient for weight at specified location\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer : int, which layer (1 = input→hidden, 2 = hidden→output)\n",
    "        from_unit : int, source unit index \n",
    "        to_unit : int, destination unit index\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        gradient : float\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.h is None:\n",
    "            raise ValueError(\"Must call forward_pass() first!\")\n",
    "        \n",
    "        if layer == 2:\n",
    "            return self._get_output_gradient(from_unit, to_unit)\n",
    "        elif layer == 1:\n",
    "            return self._get_hidden_gradient(from_unit, to_unit)\n",
    "        else:\n",
    "            raise ValueError(\"Layer must be 1 (input→hidden) or 2 (hidden→output)\")\n",
    "    \n",
    "    def _get_output_gradient(self, from_hidden, to_output):\n",
    "        \"\"\"Calculate ∂L/∂w_ij^(2) - gradient for hidden→output weight\"\"\"\n",
    "        \n",
    "        # Check bounds\n",
    "        if from_hidden >= self.n_hidden or to_output >= self.n_outputs:\n",
    "            raise ValueError(f\"Invalid indices: from_hidden={from_hidden}, to_output={to_output}\")\n",
    "        \n",
    "        # Formula: (ŷ_j - t_j) × h_i\n",
    "        target = 1 if to_output == self.target_class else 0\n",
    "        gradient = (self.y_hat[to_output] - target) * self.h[from_hidden]\n",
    "        \n",
    "        print(f\"\\n=== ∂L/∂w_{from_hidden}{to_output}^(2) ===\")\n",
    "        print(f\"Location: Hidden unit {from_hidden} → Output unit {to_output}\")\n",
    "        print(f\"Current weight: W2[{from_hidden},{to_output}] = {self.W2[from_hidden, to_output]}\")\n",
    "        print(f\"Target for output {to_output}: {target} ({'✓ correct' if target == 1 else '✗ wrong'} class)\")\n",
    "        print(f\"Formula: (ŷ_{to_output} - t_{to_output}) × h_{from_hidden}\")\n",
    "        print(f\"Values: ({self.y_hat[to_output]:.6f} - {target}) × {self.h[from_hidden]:.6f}\")\n",
    "        print(f\"Gradient: {gradient:.6f}\")\n",
    "        print(f\"Direction: {'📉 Decrease weight' if gradient < 0 else '📈 Increase weight'}\")\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def _get_hidden_gradient(self, from_input, to_hidden):\n",
    "        \"\"\"Calculate ∂L/∂w_ij^(1) - gradient for input→hidden weight\"\"\"\n",
    "        \n",
    "        # Check bounds\n",
    "        if from_input >= self.n_inputs or to_hidden >= self.n_hidden:\n",
    "            raise ValueError(f\"Invalid indices: from_input={from_input}, to_hidden={to_hidden}\")\n",
    "        \n",
    "        # Step 1: Backpropagate error from output layer\n",
    "        dL_dh = 0\n",
    "        for k in range(self.n_outputs):\n",
    "            target_k = 1 if k == self.target_class else 0\n",
    "            dL_dh += (self.y_hat[k] - target_k) * self.W2[to_hidden, k]\n",
    "        \n",
    "        # Step 2: Apply ReLU derivative\n",
    "        relu_derivative = 1 if self.z1[to_hidden] > 0 else 0\n",
    "        \n",
    "        # Step 3: Calculate delta\n",
    "        delta = dL_dh * relu_derivative\n",
    "        \n",
    "        # Step 4: Final gradient\n",
    "        gradient = delta * self.x[from_input]\n",
    "        \n",
    "        print(f\"\\n=== ∂L/∂w_{from_input}{to_hidden}^(1) ===\")\n",
    "        print(f\"Location: Input unit {from_input} → Hidden unit {to_hidden}\")\n",
    "        print(f\"Current weight: W1[{from_input},{to_hidden}] = {self.W1[from_input, to_hidden]}\")\n",
    "        print(f\"Step 1: ∂L/∂h_{to_hidden} = {dL_dh:.6f} (backprop from output)\")\n",
    "        print(f\"Step 2: ReLU'(z1_{to_hidden}) = {relu_derivative} (z1_{to_hidden} = {self.z1[to_hidden]:.6f})\")\n",
    "        print(f\"Step 3: δ_{to_hidden} = {dL_dh:.6f} × {relu_derivative} = {delta:.6f}\")\n",
    "        print(f\"Step 4: Gradient = δ_{to_hidden} × x_{from_input} = {delta:.6f} × {self.x[from_input]:.6f}\")\n",
    "        print(f\"Gradient: {gradient:.6f}\")\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def get_bias_gradient(self, layer, unit):\n",
    "        \"\"\"\n",
    "        Calculate gradient for bias term\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer : int, which layer (1 = hidden, 2 = output)\n",
    "        unit : int, which unit's bias\n",
    "        \"\"\"\n",
    "        \n",
    "        if layer == 2:\n",
    "            # Output bias gradient: ∂L/∂b_j^(2) = (ŷ_j - t_j)\n",
    "            target = 1 if unit == self.target_class else 0\n",
    "            gradient = self.y_hat[unit] - target\n",
    "            \n",
    "            print(f\"\\n=== ∂L/∂b_{unit}^(2) ===\")\n",
    "            print(f\"Output bias for unit {unit}\")\n",
    "            print(f\"Formula: ŷ_{unit} - t_{unit}\")\n",
    "            print(f\"Gradient: {self.y_hat[unit]:.6f} - {target} = {gradient:.6f}\")\n",
    "            \n",
    "        elif layer == 1:\n",
    "            # Hidden bias gradient: same as hidden weight but input = 1\n",
    "            dL_dh = 0\n",
    "            for k in range(self.n_outputs):\n",
    "                target_k = 1 if k == self.target_class else 0\n",
    "                dL_dh += (self.y_hat[k] - target_k) * self.W2[unit, k]\n",
    "            \n",
    "            relu_derivative = 1 if self.z1[unit] > 0 else 0\n",
    "            gradient = dL_dh * relu_derivative\n",
    "            \n",
    "            print(f\"\\n=== ∂L/∂b_{unit}^(1) ===\")\n",
    "            print(f\"Hidden bias for unit {unit}\")\n",
    "            print(f\"Gradient: {gradient:.6f}\")\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def show_all_gradients(self):\n",
    "        \"\"\"Show all gradients in the network\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ALL GRADIENTS IN THE NETWORK\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Output layer gradients\n",
    "        print(\"\\n🔸 HIDDEN → OUTPUT GRADIENTS:\")\n",
    "        for i in range(self.n_hidden):\n",
    "            for j in range(self.n_outputs):\n",
    "                grad = self.get_gradient(2, i, j)\n",
    "                print(f\"  ∂L/∂w_{i}{j}^(2) = {grad:.6f}\")\n",
    "        \n",
    "        # Hidden layer gradients  \n",
    "        print(\"\\n🔸 INPUT → HIDDEN GRADIENTS:\")\n",
    "        for i in range(self.n_inputs):\n",
    "            for j in range(self.n_hidden):\n",
    "                grad = self.get_gradient(1, i, j)\n",
    "                print(f\"  ∂L/∂w_{i}{j}^(1) = {grad:.6f}\")\n",
    "\n",
    "\n",
    "# ===== TEACHER'S EXAMPLE =====\n",
    "def teachers_example():\n",
    "    \"\"\"Replicate teacher's exact problem\"\"\"\n",
    "    \n",
    "    print(\"🎯 TEACHER'S PROBLEM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Define the network weights (from MATLAB code)\n",
    "    input_to_hidden = [\n",
    "        [-0.8, -0.1, 0.4],  # bias → [h1, h2, h3]\n",
    "        [0.0, 0.9, -0.1]    # x1 → [h1, h2, h3]\n",
    "    ]\n",
    "    \n",
    "    hidden_to_output = [\n",
    "        [0.0, 0.1, 0.0],    # h1 → [y1, y2, y3]\n",
    "        [0.4, -0.2, 0.8],   # h2 → [y1, y2, y3]\n",
    "        [0.0, -0.2, -0.2]   # h3 → [y1, y2, y3]\n",
    "    ]\n",
    "    \n",
    "    output_bias = [0.4, 0.5, 0.0]\n",
    "    \n",
    "    # Create network\n",
    "    net = NetworkGradientCalculator(\n",
    "        input_to_hidden_weights=input_to_hidden,\n",
    "        hidden_to_output_weights=hidden_to_output,\n",
    "        output_bias=output_bias\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    inputs = [1.0, 0.5]  # [bias, x1]\n",
    "    correct_class = 1    # y2 is correct (0-indexed)\n",
    "    \n",
    "    net.forward_pass(inputs, correct_class)\n",
    "    \n",
    "    # Calculate teacher's specific gradient\n",
    "    print(\"\\n🎯 TEACHER'S QUESTION:\")\n",
    "    gradient = net.get_gradient(layer=2, from_unit=1, to_unit=2)  # h2 → y3\n",
    "    \n",
    "    print(f\"\\nTeacher's MATLAB answer: h(2)*y(3) = {gradient:.6f}\")\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "# ===== SIMPLE EXAMPLES =====\n",
    "def simple_examples():\n",
    "    \"\"\"Simple network examples\"\"\"\n",
    "    \n",
    "    print(\"\\n🔸 SIMPLE EXAMPLE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Small 2→2→2 network\n",
    "    W1 = [\n",
    "        [-0.5, 0.3],   # bias → hidden\n",
    "        [0.8, -0.2]    # x1 → hidden\n",
    "    ]\n",
    "    \n",
    "    W2 = [\n",
    "        [0.4, -0.6],   # h1 → output\n",
    "        [0.1, 0.9]     # h2 → output\n",
    "    ]\n",
    "    \n",
    "    net = NetworkGradientCalculator(W1, W2)\n",
    "    \n",
    "    # Forward pass\n",
    "    net.forward_pass([1.0, 0.7], correct_class=0)\n",
    "    \n",
    "    # Calculate specific gradients\n",
    "    print(\"\\n📍 SPECIFIC GRADIENTS:\")\n",
    "    net.get_gradient(2, 0, 1)  # h1 → y2\n",
    "    net.get_gradient(2, 1, 0)  # h2 → y1  \n",
    "    net.get_gradient(1, 1, 0)  # x1 → h1\n",
    "\n",
    "\n",
    "# ===== YOUR CUSTOM NETWORK =====\n",
    "def my_custom_network():\n",
    "    \"\"\"\n",
    "    🔧 MODIFY THIS FUNCTION FOR YOUR OWN PROBLEMS!\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n🔧 YOUR CUSTOM NETWORK\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ✏️ DEFINE YOUR NETWORK WEIGHTS HERE:\n",
    "    my_W1 = [\n",
    "        [-1.0, 0.5, 0.2],   # bias → hidden units\n",
    "        [0.3, -0.8, 0.6],   # input 1 → hidden units\n",
    "        [0.1, 0.4, -0.3]    # input 2 → hidden units (if you have more inputs)\n",
    "    ]\n",
    "    \n",
    "    my_W2 = [\n",
    "        [0.7, -0.4],        # hidden unit 1 → outputs\n",
    "        [0.2, 0.9],         # hidden unit 2 → outputs\n",
    "        [-0.5, 0.1]         # hidden unit 3 → outputs\n",
    "    ]\n",
    "    \n",
    "    my_output_bias = [0.1, -0.2]  # Optional\n",
    "    \n",
    "    # Create your network\n",
    "    my_net = NetworkGradientCalculator(\n",
    "        input_to_hidden_weights=my_W1,\n",
    "        hidden_to_output_weights=my_W2,\n",
    "        output_bias=my_output_bias\n",
    "    )\n",
    "    \n",
    "    # ✏️ SET YOUR INPUT AND TARGET:\n",
    "    my_inputs = [1.0, 0.4, 0.8]  # [bias, input1, input2, ...]\n",
    "    my_target_class = 1          # Which output should be correct\n",
    "    \n",
    "    # Forward pass\n",
    "    my_net.forward_pass(my_inputs, my_target_class)\n",
    "    \n",
    "    # ✏️ CALCULATE THE GRADIENT YOU WANT:\n",
    "    print(\"\\n📍 YOUR GRADIENT:\")\n",
    "    my_gradient = my_net.get_gradient(\n",
    "        layer=2,        # 1 = input→hidden, 2 = hidden→output\n",
    "        from_unit=1,    # Source unit index\n",
    "        to_unit=0       # Destination unit index\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nYour gradient: {my_gradient:.6f}\")\n",
    "    \n",
    "    return my_net\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run teacher's example\n",
    "    teachers_net = teachers_example()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Run simple example\n",
    "    simple_examples()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Your custom network\n",
    "    my_net = my_custom_network()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"🎯 USAGE SUMMARY:\")\n",
    "    print(\"1. Define network: NetworkGradientCalculator(W1, W2, ...)\")\n",
    "    print(\"2. Forward pass: net.forward_pass(inputs, target_class)\")\n",
    "    print(\"3. Get gradient: net.get_gradient(layer, from_unit, to_unit)\")\n",
    "    print(\"   - layer=1: input→hidden, layer=2: hidden→output\")\n",
    "    print(\"   - Units are 0-indexed\")\n",
    "    print(\"4. Modify my_custom_network() for your problems!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37173ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02506_AIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
